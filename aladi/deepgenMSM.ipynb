{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different optimizer to finetune the learningrate better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision = 6)\n",
    "import datetime\n",
    "matplotlib.rcParams['figure.figsize'] = [6, 4]\n",
    "plt.rcParams['image.cmap'] = 'jet'\n",
    "plt.rcParams['image.interpolation']='catrom'\n",
    "from vampnet import data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "import binascii\n",
    "# import pyemma as py\n",
    "import mdshare\n",
    "import vampnet\n",
    "import mdtraj as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_whole = np.load('/group/ag_cmb/scratch/deeptime_data/alanine_data/coordinates.npy')\n",
    "dihedral = np.load('/group/ag_cmb/scratch/deeptime_data/alanine_data/dihedral.npy')\n",
    "\n",
    "# traj_whole = np.concatenate([np.sin(dihedral), np.cos(dihedral)], axis = 1)\n",
    "\n",
    "traj_data_points, input_size = traj_whole.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8000\n",
    "batch_size_gen = 1500\n",
    "#batch_size_latent = 1024\n",
    "tau = 8\n",
    "\n",
    "train_ratio = 0.9\n",
    "\n",
    "lr_koop = 1e-5\n",
    "lr_gen = 1e-4\n",
    "\n",
    "output_size = 10\n",
    "\n",
    "nodes = [100, 100, 100, 100, 100, 100, output_size]\n",
    "\n",
    "load_model = False\n",
    "load_gen_model = False\n",
    "\n",
    "res_block_len = 5\n",
    "regularizer = 1e-10\n",
    "dropout = 0.#0.5# 0.2\n",
    "\n",
    "random_size = 6\n",
    "\n",
    "sample_training_data_for_gen_before = True\n",
    "\n",
    "col_gen = 'gen'\n",
    "nodes_gen = [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, input_size]\n",
    "\n",
    "\n",
    "dict_variables = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the state to be filtered out\n",
    "# 0 top left, 1 top right, 2 bottom left, 3 bottom right\n",
    "state_to_be_filtered = 4\n",
    "\n",
    "\n",
    "# samples of the original trajectory which are closest to the center of each state\n",
    "state_centers = [8108, 133869, 0, 19678, 174924, 94042]\n",
    "\n",
    "# Filters used for each state\n",
    "filter_boxes = np.array([\n",
    "    [[-np.pi, -1.8],[ 1.8, -2.0]],\n",
    "    [[  -1.8, -0.3],[ 1.4, -2.0]],\n",
    "    [[-np.pi, -1.8],[-2.0,  1.8]],\n",
    "    [[  -1.9, -0.3],[-2.0,  1.4]],\n",
    "    [[     0,    2],[  -2,    2]],\n",
    "    [[     0,    2],[   2,   -2]]\n",
    "])\n",
    "state_filt_index = state_centers[state_to_be_filtered]\n",
    "filtbox = filter_boxes[state_to_be_filtered]\n",
    "\n",
    "\n",
    "mplt_filt1 = [[filtbox[0,0],filtbox[1,0]], filtbox[0,1] - filtbox[0,0], filtbox[1,1] - filtbox[1,0]]\n",
    "mplt_filt2 = None\n",
    "\n",
    "if filtbox[1,0] > filtbox[1,1]:\n",
    "    mplt_filt1 = [[filtbox[0,0],-np.pi], filtbox[0,1] - filtbox[0,0], filtbox[1,1] + np.pi]\n",
    "    mplt_filt2 = [[filtbox[0,0],filtbox[1,0]], filtbox[0,1] - filtbox[0,0], np.pi - filtbox[1,0]]\n",
    "\n",
    "def get_filter(tau, dih, filt_shape, circle = False):\n",
    "            \n",
    "    dht_1 = np.array(dih[:-tau].T)\n",
    "    dht_2 = np.array(dih[tau:].T)\n",
    "    \n",
    "    if circle:\n",
    "    \n",
    "        center = [-2.5,0]\n",
    "        radius = [0.8, 1]\n",
    "        \n",
    "        for dht in [dht_1, dht_2]:\n",
    "            for d, c, r in zip(dht, center, radius):\n",
    "                d -= c\n",
    "                d /= r\n",
    "\n",
    "        cond_1 = dht_1[0]**2 + dht_1[1]**2 > 1\n",
    "        cond_2 = dht_2[0]**2 + dht_2[1]**2 > 1\n",
    "        \n",
    "        indexes_filt = np.where(np.all([cond_1, cond_2], axis = 0))[0]\n",
    "    \n",
    "        return indexes_filt\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        xlim = filt_shape[0]\n",
    "        ylim1 = filt_shape[1]\n",
    "        \n",
    "        if ylim1[0] < ylim1[1]:\n",
    "            ylim2 = [0,0]\n",
    "        else:\n",
    "            ylim2  = [-np.pi, ylim1[1]]\n",
    "            ylim1[1] = np.pi\n",
    "        \n",
    "        conds = []\n",
    "        \n",
    "        for ylim in [ylim1, ylim2]:\n",
    "            for index_dht, dht in enumerate([dht_1, dht_2]):\n",
    "\n",
    "                cond_x = np.any([dht[0] < xlim[0], dht[0] > xlim[1]], axis = 0)\n",
    "                cond_y = np.any([dht[1] < ylim[0], dht[1] > ylim[1]], axis = 0)\n",
    "\n",
    "                conds.append(np.any([cond_x, cond_y], axis = 0))\n",
    "        \n",
    "        \n",
    "        indexes_filt = np.where(np.all(conds, axis = 0))[0]\n",
    "\n",
    "        return indexes_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_data = False\n",
    "\n",
    "if filter_data:\n",
    "    length_data_whole = traj_data_points - tau\n",
    "\n",
    "    indexes_filt = get_filter(tau, dihedral, filtbox)\n",
    "\n",
    "    length_data = indexes_filt.shape[0]\n",
    "\n",
    "    traj_ord = traj_whole[:-tau][indexes_filt]\n",
    "    traj_ord_lag = traj_whole[tau:][indexes_filt]\n",
    "\n",
    "    shuffle_index = np.arange(length_data)\n",
    "\n",
    "    np.random.shuffle(shuffle_index)\n",
    "\n",
    "    traj = traj_ord[shuffle_index]\n",
    "    traj_lag = traj_ord_lag[shuffle_index]\n",
    "\n",
    "    dihedral_filt = dihedral[:-tau][indexes_filt]\n",
    "\n",
    "\n",
    "else:\n",
    "    length_data = traj_data_points - tau\n",
    "\n",
    "    traj_ord = traj_whole[:-tau]\n",
    "    traj_ord_lag = traj_whole[tau:]\n",
    "\n",
    "    shuffle_index = np.arange(length_data)\n",
    "\n",
    "    np.random.shuffle(shuffle_index)\n",
    "\n",
    "    traj = traj_ord[shuffle_index]\n",
    "    traj_lag = traj_ord_lag[shuffle_index]\n",
    "\n",
    "    dihedral_filt = dihedral[:-tau]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "range_mat = np.array([[-np.pi,np.pi],[-np.pi,np.pi]])\n",
    "\n",
    "hist_im = ax.hist2d(dihedral[:,0], dihedral[:,1], bins=300, range = range_mat, norm=matplotlib.colors.LogNorm(), cmap = plt.cm.viridis)\n",
    "ax.set_xlim(-np.pi,np.pi)\n",
    "ax.set_ylim(-np.pi,np.pi)\n",
    "\n",
    "if filter_data:\n",
    "    ax.scatter(dihedral[state_filt_index,0], dihedral[state_filt_index,1], s=40, c='C3')\n",
    "    e = matplotlib.patches.Rectangle(*mplt_filt1, color='C1')\n",
    "    e.set_clip_box(ax.bbox)\n",
    "    e.set_alpha(0.2)\n",
    "    ax.add_artist(e)\n",
    "\n",
    "    if mplt_filt2:\n",
    "        e2 = matplotlib.patches.Rectangle(*mplt_filt2, color='C1')\n",
    "        e2.set_clip_box(ax.bbox)\n",
    "        e2.set_alpha(0.2)\n",
    "        ax.add_artist(e2)\n",
    "    \n",
    "font = {'fontsize': 14}\n",
    "# ax.set_xticks([-3,0,3])\n",
    "# # ax.set_yticks([-3,0,3])\n",
    "# ax.set_xticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "# ax.set_yticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "ax.set_xlabel(r'$\\phi$ [rad]', fontsize = 14)\n",
    "ax.set_ylabel(r'$\\psi$ [rad]', fontsize = 14)\n",
    "plt.colorbar(hist_im[-1], ax = ax)\n",
    "# fig.savefig('training_data_overlay_{}state.pdf'.format(state_to_be_filtered), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "range_mat = np.array([[-np.pi,np.pi],[-np.pi,np.pi]])\n",
    "\n",
    "hist_im = ax.hist2d(dihedral_filt[:,0], dihedral_filt[:,1], bins=300, range = range_mat, norm=matplotlib.colors.LogNorm(), cmap = plt.cm.viridis)\n",
    "if filter_data:\n",
    "    ax.scatter(dihedral[state_filt_index,0], dihedral[state_filt_index,1], s=40, c='C3')\n",
    "    \n",
    "ax.set_xlim(-np.pi,np.pi)\n",
    "ax.set_ylim(-np.pi,np.pi)\n",
    "font = {'fontsize': 14}\n",
    "ax.set_xticks([-3,0,3])\n",
    "ax.set_yticks([-3,0,3])\n",
    "ax.set_xticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "ax.set_yticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "ax.set_xlabel(r'$\\phi$ [rad]', fontsize = 14)\n",
    "ax.set_ylabel(r'$\\psi$ [rad]', fontsize = 14)\n",
    "plt.colorbar(hist_im[-1], ax = ax)\n",
    "fig.savefig('training_data_removed_{}state.pdf'.format(state_to_be_filtered), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train = int(np.floor(length_data * train_ratio))\n",
    "length_vali = length_data - length_train\n",
    "\n",
    "traj_data_train = traj[:length_train]\n",
    "traj_data_train_lag = traj_lag[:length_train]\n",
    "\n",
    "traj_data_valid = traj[length_train:]\n",
    "traj_data_valid_lag = traj_lag[length_train:]\n",
    "\n",
    "# Input of the first network\n",
    "X1_train = traj_data_train.astype('float32')\n",
    "X2_train  = traj_data_train_lag.astype('float32')\n",
    "\n",
    "# Input for validation\n",
    "X1_vali = traj_data_valid.astype('float32')\n",
    "X2_vali = traj_data_valid_lag.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_layer(in_size, out_size, col):\n",
    "    w = tf.get_variable(shape = [in_size, out_size], initializer=tf.contrib.layers.xavier_initializer(), name='W', collections = [col])\n",
    "    b = tf.get_variable(initializer=tf.constant(0., shape = [out_size]), name='b', collections = [col])\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_func(nodes, acti = tf.nn.elu, output_acti = tf.nn.softmax):\n",
    "    layers_list = [tf.layers.BatchNormalization()]\n",
    "    for node in nodes[:-1]:\n",
    "        layers_list.append(tf.layers.Dense(node, activation=acti))\n",
    "    layers_list.append(tf.layers.Dense(nodes[-1], activation=output_acti))\n",
    "    return layers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi(x_t, layers, name='chi'):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        l = tf.identity(x_t, name = 'input')\n",
    "        \n",
    "        for layer in layers:\n",
    "            l = layer(l)\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma(x_t, layers, name='gamma'):\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        l = tf.identity(x_t, name = 'input')\n",
    "        \n",
    "        for layer in layers:\n",
    "            l = layer(l)\n",
    "        \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_generator(nodes, acti=tf.nn.elu, res_dept=3, drop=0.):\n",
    "    layers_list = []\n",
    "    for node in nodes[:-1]:\n",
    "        for res in range(res_dept):\n",
    "            layers_list.append(tf.layers.BatchNormalization())\n",
    "            layers_list.append(tf.layers.Dense(node, activation=acti))\n",
    "            layers_list.append(tf.layers.Dropout(rate=drop))\n",
    "    layers_list.append(tf.layers.BatchNormalization())\n",
    "    layers_list.append(tf.layers.Dense(nodes[-1]))\n",
    "    return layers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(x_t, layers, fac_res, block_len=3, acti=tf.nn.elu, name='generator'):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        batch_size = tf.shape(x_t)[0]\n",
    "        rand_input = tf.random_normal(shape = [batch_size, random_size], name = 'random_input')\n",
    "        # find the state at t\n",
    "        rand_num = tf.random_uniform(tf.expand_dims(batch_size, axis=0), minval=0, maxval=1)\n",
    "        state = output_size - tf.reduce_sum(tf.to_int32(tf.cumsum(x_t, axis=1) >= rand_num), axis=1)\n",
    "        \n",
    "#         print(state)\n",
    "        x_sample = tf.gather_nd(tf.eye(output_size), tf.expand_dims(state,axis=1))\n",
    "        \n",
    "        \n",
    "        \n",
    "        l = tf.concat([x_t, rand_input], axis = -1, name = 'input')\n",
    "        \n",
    "        counter = 0\n",
    "#         l = layers[counter](l) # first batchnorm\n",
    "#         counter += 1\n",
    "        \n",
    "        number_res_layers = (len(layers)-2) // block_len // 3 # -2 because output and first batchnorm\n",
    "        \n",
    "        for res_layer in range(number_res_layers):\n",
    "            for _ in range(3): #  batchnorm, Dense, drop\n",
    "                l = layers[counter](l)\n",
    "                counter += 1\n",
    "            l_res = tf.identity(l)\n",
    "            for _ in range(block_len-1): # res block same size\n",
    "                for _ in range(3):  # batchnorm, Dense, drop \n",
    "                    l_res = layers[counter](l_res)\n",
    "                    counter += 1\n",
    "            \n",
    "            # combine res\n",
    "            l = acti(tf.add(fac_res * l, l_res))\n",
    "        l = layers[counter](l) # batchnorm\n",
    "        output = layers[counter+1](l)\n",
    "        \n",
    "        return output\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "dict_variables = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_chi = layers_func(nodes)\n",
    "layers_gamma = layers_func(nodes, output_acti=tf.nn.relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = tf.placeholder(tf.float32, shape = [None, input_size], name = 'input_t')\n",
    "x_tau = tf.placeholder(tf.float32, shape = [None, input_size], name = 'input_tau')\n",
    "keep_prob_drop = tf.placeholder_with_default(1., shape = (), name = 'prob_drop')\n",
    "\n",
    "layers_gen = layers_generator(nodes_gen, drop=1.-keep_prob_drop)\n",
    "\n",
    "chi_out = chi(x_t, layers_chi)\n",
    "gamma_out = gamma(x_tau, layers_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_chi_tau = tf.placeholder(tf.float32, shape = (None, output_size), name = 'input_gen')\n",
    "gen_chi_tau2 = tf.placeholder(tf.float32, shape = (None, output_size), name = 'input_gen')\n",
    "gen_x_tau = tf.placeholder(tf.float32, shape = (None, input_size), name = 'label_gen')\n",
    "k_tf = tf.placeholder(tf.float32, shape = (output_size, output_size), name = 'koop_op_gen')\n",
    "fac_res_tf = tf.placeholder_with_default(0., shape=(), name='factor_res')\n",
    "\n",
    "\n",
    "\n",
    "gen_x1 = generator(gen_chi_tau, layers_gen, fac_res_tf, res_block_len)\n",
    "gen_x2 = generator(gen_chi_tau2, layers_gen, fac_res_tf, res_block_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_chi = []\n",
    "var_gamma = []\n",
    "for i in range(len(layers_chi)):\n",
    "    var_chi += layers_chi[i].variables\n",
    "    var_gamma += layers_gamma[i].variables\n",
    "var_gen = []\n",
    "for i in range(len(layers_gen)):\n",
    "    var_gen += layers_gen[i].variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_match_loss(x_tau, y, y_bar):\n",
    "\n",
    "    norm_01 = tf.norm(x_tau - y, axis = 1)\n",
    "    norm_02 = tf.norm(x_tau - y_bar, axis = 1)\n",
    "    norm_12 = tf.norm(y - y_bar, axis = 1)\n",
    "    batch_size_temp = tf.shape(x_tau)[0]\n",
    "    ret = 1./tf.cast(batch_size_temp, tf.float32)*tf.reduce_sum(norm_01+norm_02-norm_12, axis=0)\n",
    "    return ret \n",
    "\n",
    "def vampe_loss(chi, gamma):\n",
    "    b = tf.to_float(tf.shape(chi)[0])\n",
    "    c00 = 1/b*(tf.transpose(chi) @ chi)\n",
    "    c11 = 1/b*(tf.transpose(gamma) @ gamma)\n",
    "    c01 = 1/b*(tf.transpose(chi) @ gamma)\n",
    "\n",
    "    gamma_dia_inv = tf.diag(1/(tf.reduce_mean(gamma, axis = 0)+ regularizer))  # add something so no devide by zero\n",
    "\n",
    "    first_term = c00 @ gamma_dia_inv @ c11 @ gamma_dia_inv\n",
    "    second_term = 2 * (c01 @ gamma_dia_inv)\n",
    "    vampe_arg = first_term - second_term\n",
    "    vampe = tf.trace(vampe_arg)\n",
    "    \n",
    "    return vampe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Losses'):\n",
    "    \n",
    "    with tf.name_scope('generator'):\n",
    "        loss_gen = energy_match_loss(gen_x_tau, gen_x1, gen_x2)\n",
    "\n",
    "    with tf.name_scope('koopman'):\n",
    "        loss_vampe = vampe_loss(chi_out, gamma_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizers'):\n",
    "    with tf.name_scope('koopman'):\n",
    "        var_koop = var_chi + var_gamma\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=lr_koop)\n",
    "        gvs = opt.compute_gradients(loss_vampe, var_list = var_koop)\n",
    "        crapped_gvs = [(tf.clip_by_value(grad, -1.,1.), var) for grad, var in gvs]\n",
    "        solver_koop = opt.apply_gradients(crapped_gvs)\n",
    "        var_adam_koop = opt.variables()\n",
    "    \n",
    "    with tf.name_scope('gam_opt'):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=lr_koop)\n",
    "        gvs = opt.compute_gradients(loss_vampe, var_list = var_gamma)\n",
    "        crapped_gvs = [(tf.clip_by_value(grad, -1.,1.), var) for grad, var in gvs]\n",
    "        solver_gamma = opt.apply_gradients(crapped_gvs)\n",
    "        var_adam_gam = opt.variables()\n",
    "    \n",
    "    with tf.name_scope('generator'):\n",
    "        lr_tensor = tf.placeholder(tf.float32, shape = (), name = 'leanrning_rate')\n",
    "#         var_gen = tf.get_collection(col_gen)\n",
    "        \n",
    "        opt = tf.train.AdamOptimizer(learning_rate=lr_gen)\n",
    "        gvs = opt.compute_gradients(loss_gen, var_list = var_gen)\n",
    "        crapped_gvs = [(tf.clip_by_value(grad, -1.,1.), var) for grad, var in gvs]\n",
    "        solver_gen = opt.apply_gradients(crapped_gvs)\n",
    "        var_adam_gen = opt.variables()\n",
    "        \n",
    "        opt_sg = tf.train.GradientDescentOptimizer(learning_rate=lr_tensor)\n",
    "        gvs_sg = opt_sg.compute_gradients(loss_gen, var_list = var_gen)\n",
    "        crapped_gvs_sg = [(tf.clip_by_value(grad, -1.,1.), var) for grad, var in gvs_sg]\n",
    "        solver_gen_sg = opt.apply_gradients(crapped_gvs_sg)\n",
    "        var_adam_gen += opt.variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('initializers'):\n",
    "    init_var_koop = tf.variables_initializer(var_koop)\n",
    "    init_var_gamma = tf.variables_initializer(var_gamma)\n",
    "    init_global_var = tf.global_variables_initializer()\n",
    "    init_var_gen = tf.variables_initializer(var_gen)\n",
    "    \n",
    "    init_adam_koop = tf.variables_initializer(var_adam_koop)\n",
    "    init_adam_gen = tf.variables_initializer(var_adam_gen)\n",
    "    init_adam_gam = tf.variables_initializer(var_adam_gam)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "def reset_weights_koop():\n",
    "    sess.run([init_var_koop, init_adam_koop])\n",
    "\n",
    "reset_weights_koop()\n",
    "\n",
    "writer = tf.summary.FileWriter(logdir = './tensorboard_log/', graph = tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batch(batchsize, index):\n",
    "    \n",
    "    start = index*batchsize\n",
    "    end = (index+1)*batchsize\n",
    "    \n",
    "    if(end < length_train):\n",
    "        batch_traj = X1_train[start : end]\n",
    "        batch_traj_lag = X2_train[start:end]\n",
    "        \n",
    "    else:\n",
    "        batch_traj = X1_train[start:]\n",
    "        batch_traj_lag = X2_train[start:]\n",
    "    \n",
    "    return batch_traj, batch_traj_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_weights_koop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainings_completed = 0\n",
    "batches = np.ceil(length_train/batch_size).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver_koop = tf.train.Saver(var_koop+var_adam_koop)\n",
    "if load_model:\n",
    "    if filter_data:\n",
    "        saver_koop.restore(sess, \"./saved_models/working_alavamp_filter_state{}.ckpt\".format(state_to_be_filtered))\n",
    "    else:\n",
    "        saver_koop.restore(sess, '/group/ag_cmb/scratch/deeptime_data/data_nips/ML_koopman_working_alavamp.ckpt')\n",
    "\n",
    "if False:\n",
    "    save_path = saver_koop.save(sess, \"./saved_models/working_alavamp_filter_state{}.ckpt\".format(state_to_be_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_vamp_score = 0.\n",
    "if not load_model:\n",
    "    while best_vamp_score > -2.95:\n",
    "        reset_weights_koop()\n",
    "        epochs = 801\n",
    "        plot_save_every = 25\n",
    "        offset = trainings_completed\n",
    "\n",
    "        # compute distribution of training data\n",
    "        output_nodes_list = [solver_koop, \n",
    "                             loss_vampe,\n",
    "                                  ]\n",
    "        flag = False\n",
    "        for run in range(epochs):\n",
    "            if flag:\n",
    "                break    \n",
    "            loss_curr = []\n",
    "            loss_vali_curr = []\n",
    "\n",
    "            for batch in range(batches):\n",
    "                # Get data for minibatch\n",
    "                batch_traj, batch_traj_lag = get_mini_batch(batch_size, batch)\n",
    "\n",
    "\n",
    "                feed_dict_D = {x_t: batch_traj,\n",
    "                               x_tau: batch_traj_lag,\n",
    "                               keep_prob_drop : (1.-dropout)\n",
    "                              }\n",
    "\n",
    "                # Create Input for Training D\n",
    "                # Train Dyn\n",
    "                _, loss_temp1 = sess.run(output_nodes_list,\n",
    "                                                feed_dict=feed_dict_D)\n",
    "                loss_curr.append(loss_temp1)\n",
    "                # Write to Summary\n",
    "            loss_vali_temp = sess.run(loss_vampe, feed_dict={x_t : X1_vali,\n",
    "                                                           x_tau :  X2_vali,\n",
    "                                                           keep_prob_drop : 1.})\n",
    "\n",
    "        #         if (np.isnan(loss_temp1)):\n",
    "        #             print('found NAN in batch {}'.format(batch))\n",
    "        #             flag = True\n",
    "        #             break\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "            loss_np = np.mean(loss_curr)\n",
    "            loss_vali = loss_vali_temp\n",
    "\n",
    "        #     n1_all.append(liste_n1)\n",
    "        #     n2_all.append(liste_n2)\n",
    "        #     n12_all.append(liste_n12)\n",
    "            # Create a new Summary object with your measure\n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag=\"loss\", simple_value=loss_vali)\n",
    "\n",
    "            # Add it to the Tensorboard summary writer\n",
    "            # Make sure to specify a step parameter to get nice graphs over time\n",
    "            writer.add_summary(summary, run+offset)  \n",
    "\n",
    "            if run % plot_save_every == 0:\n",
    "                print('Iter: {}'.format(run))\n",
    "                print('Loss: {:.4}'.format(loss_np))\n",
    "                print('Loss vali: {:.4}'.format(loss_vali))\n",
    "                print()\n",
    "\n",
    "        trainings_completed += epochs\n",
    "        pred = sess.run(chi_out, feed_dict = {x_t: traj_whole, keep_prob_drop : 1.})\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        range_mat = np.array([[-np.pi,np.pi],[-np.pi,np.pi]])\n",
    "\n",
    "        maxi_train = np.max(pred, axis= 1)\n",
    "        coor_train = np.zeros_like(pred)\n",
    "        for i in range(output_size):\n",
    "            coor_train = np.where(pred[:,i]== maxi_train)[0]\n",
    "            ax.scatter(dihedral[coor_train,0], dihedral[coor_train,1], s=1)\n",
    "\n",
    "        ax.set_xlim(-np.pi,np.pi)\n",
    "        ax.set_ylim(-np.pi,np.pi)\n",
    "\n",
    "        if filter_data:\n",
    "            e = matplotlib.patches.Rectangle(*mplt_filt1, color='k')\n",
    "            e.set_clip_box(ax.bbox)\n",
    "            e.set_alpha(0.2)\n",
    "            ax.add_artist(e)\n",
    "\n",
    "        font = {'fontsize': 14}\n",
    "        ax.set_xticks([-3,0,3])\n",
    "        ax.set_yticks([-3,0,3])\n",
    "        ax.set_xticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "        ax.set_yticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "        ax.set_xlabel(r'$\\phi$ [rad]', fontsize = 14)\n",
    "        ax.set_ylabel(r'$\\psi$ [rad]', fontsize = 14)\n",
    "        plt.show()\n",
    "        best_vamp_score = loss_vali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sess.run(chi_out, feed_dict = {x_t: traj_whole, keep_prob_drop : 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "range_mat = np.array([[-np.pi,np.pi],[-np.pi,np.pi]])\n",
    "\n",
    "maxi_train = np.max(pred, axis= 1)\n",
    "coor_train = np.zeros_like(pred)\n",
    "for i in range(output_size):\n",
    "    coor_train = np.where(pred[:,i]== maxi_train)[0]\n",
    "    ax.scatter(dihedral[coor_train,0], dihedral[coor_train,1], s=1)\n",
    "    \n",
    "ax.set_xlim(-np.pi,np.pi)\n",
    "ax.set_ylim(-np.pi,np.pi)\n",
    "\n",
    "if filter_data:\n",
    "    e = matplotlib.patches.Rectangle(*mplt_filt1, color='k')\n",
    "    e.set_clip_box(ax.bbox)\n",
    "    e.set_alpha(0.2)\n",
    "    ax.add_artist(e)\n",
    "    \n",
    "font = {'fontsize': 14}\n",
    "ax.set_xticks([-3,0,3])\n",
    "ax.set_yticks([-3,0,3])\n",
    "ax.set_xticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "ax.set_yticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "ax.set_xlabel(r'$\\phi$ [rad]', fontsize = 14)\n",
    "ax.set_ylabel(r'$\\psi$ [rad]', fontsize = 14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_K_vampe(traj_ord_lag):\n",
    "\n",
    "    chi_xtau = sess.run(chi_out, feed_dict = {x_t: traj_ord_lag, keep_prob_drop : 1.})\n",
    "    gamma_xtau = sess.run(gamma_out, feed_dict = {x_tau: traj_ord_lag, keep_prob_drop : 1.})\n",
    "\n",
    "    gamma_mean = 1/(np.mean(gamma_xtau, axis = 0) + regularizer)\n",
    "\n",
    "    K = np.empty((output_size, output_size))\n",
    "\n",
    "    for i in range(output_size):\n",
    "        for j in range(output_size):\n",
    "            K[i,j] = np.mean(gamma_mean[i]*gamma_xtau[:,i]*chi_xtau[:,j], axis = 0)\n",
    "            \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = estimate_K_vampe(traj_ord_lag)\n",
    "\n",
    "eigv, eigw = np.linalg.eig(K)\n",
    "\n",
    "eig_sort = np.argsort(eigv)[::-1]\n",
    "\n",
    "eigv = eigv[eig_sort]\n",
    "eigw = eigw[:, eig_sort]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights_gen():\n",
    "    sess.run([init_var_gen, init_adam_gen])\n",
    "\n",
    "reset_weights_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_ord = sess.run(chi_out, feed_dict = {x_t: traj_ord, keep_prob_drop : 1.})\n",
    "chi_ord_lag = sess.run(chi_out, feed_dict = {x_t: traj_ord_lag, keep_prob_drop : 1.})\n",
    "\n",
    "chi_shuffle = chi_ord[shuffle_index]\n",
    "chi_shuffle_lag = chi_ord_lag[shuffle_index]\n",
    "\n",
    "chi_data_train = chi_shuffle[:length_train]\n",
    "chi_data_train_lag = chi_shuffle_lag[:length_train]\n",
    "\n",
    "chi_data_valid = chi_shuffle[length_train:]\n",
    "chi_data_valid_lag = chi_shuffle_lag[length_train:]\n",
    "\n",
    "# Input of the first network\n",
    "chi_X1_train = chi_data_train.astype('float32')\n",
    "chi_X2_train  = chi_data_train_lag.astype('float32')\n",
    "\n",
    "# Input for validation\n",
    "chi_X1_vali = chi_data_valid.astype('float32')\n",
    "chi_X2_vali = chi_data_valid_lag.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_dist(dist, arg_max = False, cumsum = False):\n",
    "    sample_num, sample_size = dist.shape\n",
    "    drawn_values = np.random.rand(sample_num)\n",
    "    sample_retval = np.zeros_like(dist)\n",
    "    if cumsum:\n",
    "        sample_cumsum = dist\n",
    "    else:\n",
    "        sample_cumsum = np.cumsum(dist, axis = 1)\n",
    "    \n",
    "    sample_diff = sample_cumsum - np.tile(drawn_values,(sample_size,1)).T\n",
    "    sample_sign = np.sign(sample_diff)\n",
    "    sample_argmax = np.argmax(sample_sign, axis = 1)\n",
    "    \n",
    "    if arg_max: \n",
    "        return sample_argmax\n",
    "    sample_retval[np.arange(sample_num), sample_argmax] = 1\n",
    "    \n",
    "    return sample_retval\n",
    "\n",
    "def get_mini_batch_gen(batchsize, index):\n",
    "    \n",
    "    start = index*batchsize\n",
    "    end = (index+1)*batchsize\n",
    "        \n",
    "    if(end < length_train):\n",
    "            \n",
    "        \n",
    "        batch_traj = chi_X1_train[start : end]\n",
    "        \n",
    "        batch_traj_lag = X2_train[start : end]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        batch_traj = chi_X1_train[start:]\n",
    "       \n",
    "        \n",
    "        batch_traj_lag = X2_train[start:]\n",
    "        \n",
    "        \n",
    "    \n",
    "    return batch_traj, batch_traj_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainings_completed_gen = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run([init_var_gen, init_adam_gen, init_adam_gam]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size_gen = 256 # not over 2100 because the lowest populated is\n",
    "batches = np.ceil(length_train/batch_size_gen).astype('int')\n",
    "best_value = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample training data before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(var_koop + var_gen)\n",
    "\n",
    "if load_gen_model:\n",
    "    saver.restore(sess, \"./saved_models/best_ala_gen_model_equal.ckpt\")\n",
    "    \n",
    "    if False:\n",
    "        save_path = saver.save(sess, \"./saved_models/best_ala_gen_model_equal.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor back to old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 501\n",
    "plot_save_every = 25\n",
    "offset = trainings_completed_gen\n",
    "\n",
    "stochastic_gradient = False\n",
    "lr_gentemp = lr_gen / 100000.\n",
    "\n",
    "# compute distribution of training data\n",
    "if stochastic_gradient:\n",
    "    output_nodes_list = [solver_gen_sg, \n",
    "#                      loss_gen,\n",
    "                          ]\n",
    "else:\n",
    "    output_nodes_list = [solver_gen, \n",
    "#                      loss_gen,\n",
    "                          ]\n",
    "flag = False\n",
    "fac_res_temp = 1.\n",
    "fac_smaller = 1. /( (epochs // 100) - 1) # so it is 0 for the last 100 epochs\n",
    "\n",
    "# fac_res_temp = 0.\n",
    "# fac_smaller = 0.\n",
    "\n",
    "for run in range(epochs):\n",
    "    if flag:\n",
    "        break    \n",
    "    loss_curr = []\n",
    "    loss_vali_curr = []\n",
    "    if (run +1) % 100 == 0:\n",
    "        fac_res_temp -= fac_smaller\n",
    "    \n",
    "    for batch in range(batches):\n",
    "        # Get data for minibatch\n",
    "        batch_traj, batch_traj_lag = get_mini_batch_gen(batch_size_gen, batch)\n",
    "        \n",
    "        if stochastic_gradient:\n",
    "            training_dict = {gen_chi_tau: batch_traj,\n",
    "                             gen_chi_tau2: batch_traj,\n",
    "                           gen_x_tau: batch_traj_lag,\n",
    "                           keep_prob_drop : (1.-dropout),\n",
    "                             lr_tensor : lr_gentemp,\n",
    "                             fac_res_tf : fac_res_temp\n",
    "                          }\n",
    "        else:\n",
    "            training_dict = {gen_chi_tau: batch_traj,\n",
    "                             gen_chi_tau2: batch_traj,\n",
    "                           gen_x_tau: batch_traj_lag,\n",
    "                           keep_prob_drop : (1.-dropout),\n",
    "                             fac_res_tf : fac_res_temp\n",
    "                          }\n",
    "\n",
    "        # Create Input for Training D\n",
    "        # Train Dyn\n",
    "        _ = sess.run(output_nodes_list,\n",
    "                                        feed_dict=training_dict)\n",
    "    for batch in range(batches):\n",
    "        batch_traj, batch_traj_lag = get_mini_batch_gen(batch_size_gen, batch)\n",
    "        training_dict = {gen_chi_tau: batch_traj,\n",
    "                         gen_chi_tau2: batch_traj,\n",
    "                       gen_x_tau: batch_traj_lag,\n",
    "                       keep_prob_drop : (1.-dropout),\n",
    "                         fac_res_tf : fac_res_temp\n",
    "                      }\n",
    "        loss_temp1 = sess.run(loss_gen, feed_dict=training_dict)\n",
    "        # Write to Summary\n",
    "#         validation_dict = {gen_chi_tau : sample_from_dist(chi_X1_vali @ K),\n",
    "#                            gen_x_tau :  X2_vali,\n",
    "#                            keep_prob_drop : 1.}\n",
    "#         loss_vali_temp = sess.run(loss_gen, feed_dict=validation_dict)\n",
    "\n",
    "#         if (np.isnan(loss_temp1)):\n",
    "#             print('found NAN in batch {}'.format(batch))\n",
    "#             flag = True\n",
    "#             break\n",
    "            \n",
    "        loss_curr.append(loss_temp1)\n",
    "#         loss_vali_curr.append(loss_vali_temp)\n",
    "        \n",
    "    loss_np = np.mean(loss_curr)\n",
    "    if loss_np < best_value :\n",
    "        best_value = loss_np\n",
    "#         if conditioned_dist:\n",
    "#             print('Save cond: {}'.format(best_value))\n",
    "#             save_path = saver.save(sess, \"./saved_models/best_ala_gen_model_test2.ckpt\")\n",
    "#         else:\n",
    "#             save_path = saver.save(sess, \"./saved_models/best_ala_gen_model_test2_equal.ckpt\")\n",
    "#     loss_vali = np.mean(loss_vali_curr)\n",
    "\n",
    "    # Create a new Summary object with your measure\n",
    "    summary = tf.Summary()\n",
    "    summary.value.add(tag=\"loss_gen_equal\", simple_value=loss_np)\n",
    "\n",
    "    # Add it to the Tensorboard summary writer\n",
    "    # Make sure to specify a step parameter to get nice graphs over time\n",
    "    writer.add_summary(summary, run+offset)  \n",
    "    \n",
    "    if run % plot_save_every == 0:\n",
    "        print('Iter: {}'.format(run))\n",
    "        print('Loss: {:.4}'.format(loss_np))\n",
    "#         print('Loss vali: {:.4}'.format(loss_vali))\n",
    "        print()\n",
    "        \n",
    "trainings_completed_gen += epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 201\n",
    "plot_save_every = 25\n",
    "offset = trainings_completed_gen\n",
    "\n",
    "stochastic_gradient = False\n",
    "lr_gentemp = lr_gen / 100000.\n",
    "\n",
    "# compute distribution of training data\n",
    "if stochastic_gradient:\n",
    "    output_nodes_list = [solver_gen_sg, \n",
    "#                      loss_gen,\n",
    "                          ]\n",
    "else:\n",
    "    output_nodes_list = [solver_gen, \n",
    "#                      loss_gen,\n",
    "                          ]\n",
    "flag = False\n",
    "# fac_res_temp = 1.\n",
    "# fac_smaller = 1. /( (epochs // 100) - 1) # so it is 0 for the last 100 epochs\n",
    "\n",
    "fac_res_temp = 0.\n",
    "fac_smaller = 0.\n",
    "\n",
    "for run in range(epochs):\n",
    "    if flag:\n",
    "        break    \n",
    "    loss_curr = []\n",
    "    loss_vali_curr = []\n",
    "    if (run +1) % 100 == 0:\n",
    "        fac_res_temp -= fac_smaller\n",
    "    \n",
    "    for batch in range(batches):\n",
    "        # Get data for minibatch\n",
    "        batch_traj, batch_traj_lag = get_mini_batch_gen(batch_size_gen, batch)\n",
    "        \n",
    "        if stochastic_gradient:\n",
    "            training_dict = {gen_chi_tau: batch_traj,\n",
    "                             gen_chi_tau2: batch_traj,\n",
    "                           gen_x_tau: batch_traj_lag,\n",
    "                           keep_prob_drop : (1.-dropout),\n",
    "                             lr_tensor : lr_gentemp,\n",
    "                             fac_res_tf : fac_res_temp\n",
    "                          }\n",
    "        else:\n",
    "            training_dict = {gen_chi_tau: batch_traj,\n",
    "                             gen_chi_tau2: batch_traj,\n",
    "                           gen_x_tau: batch_traj_lag,\n",
    "                           keep_prob_drop : (1.-dropout),\n",
    "                             fac_res_tf : fac_res_temp\n",
    "                          }\n",
    "\n",
    "        # Create Input for Training D\n",
    "        # Train Dyn\n",
    "        _ = sess.run(output_nodes_list,\n",
    "                                        feed_dict=training_dict)\n",
    "    for batch in range(batches):\n",
    "        batch_traj, batch_traj_lag = get_mini_batch_gen(batch_size_gen, batch)\n",
    "        training_dict = {gen_chi_tau: batch_traj,\n",
    "                         gen_chi_tau2: batch_traj,\n",
    "                       gen_x_tau: batch_traj_lag,\n",
    "                       keep_prob_drop : (1.-dropout),\n",
    "                         fac_res_tf : fac_res_temp\n",
    "                      }\n",
    "        loss_temp1 = sess.run(loss_gen, feed_dict=training_dict)\n",
    "        # Write to Summary\n",
    "#         validation_dict = {gen_chi_tau : sample_from_dist(chi_X1_vali @ K),\n",
    "#                            gen_x_tau :  X2_vali,\n",
    "#                            keep_prob_drop : 1.}\n",
    "#         loss_vali_temp = sess.run(loss_gen, feed_dict=validation_dict)\n",
    "\n",
    "#         if (np.isnan(loss_temp1)):\n",
    "#             print('found NAN in batch {}'.format(batch))\n",
    "#             flag = True\n",
    "#             break\n",
    "            \n",
    "        loss_curr.append(loss_temp1)\n",
    "#         loss_vali_curr.append(loss_vali_temp)\n",
    "        \n",
    "    loss_np = np.mean(loss_curr)\n",
    "    if loss_np < best_value :\n",
    "        best_value = loss_np\n",
    "#         if conditioned_dist:\n",
    "#             print('Save cond: {}'.format(best_value))\n",
    "#             save_path = saver.save(sess, \"./saved_models/best_ala_gen_model_test2.ckpt\")\n",
    "#         else:\n",
    "#             save_path = saver.save(sess, \"./saved_models/best_ala_gen_model_test2_equal.ckpt\")\n",
    "#     loss_vali = np.mean(loss_vali_curr)\n",
    "\n",
    "    # Create a new Summary object with your measure\n",
    "    summary = tf.Summary()\n",
    "    summary.value.add(tag=\"loss_gen_equal\", simple_value=loss_np)\n",
    "\n",
    "    # Add it to the Tensorboard summary writer\n",
    "    # Make sure to specify a step parameter to get nice graphs over time\n",
    "    writer.add_summary(summary, run+offset)  \n",
    "    \n",
    "    if run % plot_save_every == 0:\n",
    "        print('Iter: {}'.format(run))\n",
    "        print('Loss: {:.4}'.format(loss_np))\n",
    "#         print('Loss vali: {:.4}'.format(loss_vali))\n",
    "        print()\n",
    "        \n",
    "trainings_completed_gen += epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate cond. probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_points_in_square(dih, c, l):\n",
    "    \n",
    "            \n",
    "    dht = np.array(dih.T)\n",
    "    \n",
    "    xlim = np.array([c[0]-l/2,c[0]+l/2])\n",
    "    ylim1 = np.array([c[1]-l/2,c[1]+l/2])\n",
    "    \n",
    "    if ylim1[1] < np.pi and ylim1[0] > -np.pi:\n",
    "        filters_num = 1\n",
    "        ylim2 = None\n",
    "        \n",
    "    elif ylim1[0] < -np.pi:\n",
    "        filters_num = 2\n",
    "        ylim2  = np.array([2*np.pi + ylim1[0], np.pi])\n",
    "        ylim1[0] = - np.pi\n",
    "        \n",
    "    else:\n",
    "        ylim2  = np.array([-np.pi, -2*np.pi + ylim1[1]])\n",
    "        ylim1[1] = np.pi\n",
    "        filters_num = 2\n",
    "\n",
    "    conds = np.empty((dht.shape[1], filters_num))\n",
    "\n",
    "    \n",
    "    for index_ylim, ylim in enumerate([ylim1, ylim2]):       \n",
    "        if index_ylim < filters_num:\n",
    "            \n",
    "            cond_1 = dht[0] < xlim[0]\n",
    "            cond_2 = dht[0] > xlim[1]\n",
    "            \n",
    "            cond_3 = dht[1] < ylim[0]\n",
    "            cond_4 = dht[1] > ylim[1]\n",
    "\n",
    "            conds[:,index_ylim] = np.any([cond_1, cond_2, cond_3, cond_4], axis = 0)\n",
    "    \n",
    "    indexes_filt = np.where(np.logical_not(conds))[0]\n",
    "\n",
    "    return indexes_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dihedral_lag = dihedral[tau:]\n",
    "gen_cond_points = np.zeros((6), dtype = int)\n",
    "\n",
    "for r in range(6):\n",
    "    \n",
    "    filt_center =  dihedral[state_centers[r]]\n",
    "    filt_rad = 0.7\n",
    "    \n",
    "    dih_center = get_points_in_square(dihedral[:-tau], filt_center, filt_rad)\n",
    "    gen_cond_points[r] = len(dih_center)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    range_mat = np.array([[-np.pi,np.pi],[-np.pi,np.pi]])\n",
    "\n",
    "    hist_im = ax.hist2d(*dihedral_lag[dih_center].T, bins=100, range = range_mat, norm=matplotlib.colors.LogNorm(), cmap = plt.cm.viridis, normed = True)\n",
    "    ax.scatter(*dihedral[state_centers[r]], s=40, c='C3')\n",
    "    \n",
    "    e = matplotlib.patches.Rectangle(filt_center-filt_rad/2, filt_rad, filt_rad, color='C1')\n",
    "    e.set_clip_box(ax.bbox)\n",
    "    e.set_alpha(0.4)\n",
    "    ax.add_artist(e)\n",
    "    \n",
    "    if r == 5:\n",
    "        new_fc = filt_center-filt_rad/2 + [0, 2*np.pi]\n",
    "        e2 = matplotlib.patches.Rectangle(new_fc, filt_rad, filt_rad, color='C1')\n",
    "        e2.set_clip_box(ax.bbox)\n",
    "        e2.set_alpha(0.4)\n",
    "        ax.add_artist(e2)\n",
    "        \n",
    "    \n",
    "    ax.set_xlim(-np.pi,np.pi)\n",
    "    ax.set_ylim(-np.pi,np.pi)\n",
    "    font = {'fontsize': 14}\n",
    "    ax.set_xticks([-3.1,0,3.1])\n",
    "    ax.set_yticks([-3.1,0,3.1])\n",
    "    ax.set_xticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "    ax.set_yticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "    ax.set_xlabel(r'$\\phi$ [rad]', fontsize = 14)\n",
    "    ax.set_ylabel(r'$\\psi$ [rad]', fontsize = 14)\n",
    "#     fig.savefig('cond_dist_real_state{}.pdf'.format(r), bbox_inches='tight')\n",
    "\n",
    "#     plt.colorbar(hist_im[-1], ax = ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_points = 6\n",
    "cond_points = gen_cond_points.max()\n",
    "\n",
    "\n",
    "\n",
    "predict_cond = np.empty((grid_points, cond_points, input_size))\n",
    "\n",
    "starting_pools = []\n",
    "\n",
    "for r in range(grid_points):\n",
    "\n",
    "    filt_center =  dihedral[state_centers[r]]\n",
    "    filt_rad = 0.7\n",
    "\n",
    "    dih_center = get_points_in_square(dihedral[:-tau], filt_center, filt_rad)\n",
    "    starting_pools.append(dih_center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_cond in range(cond_points):\n",
    "\n",
    "    indexes_best_dih = np.empty((grid_points), dtype = int)\n",
    "    for r in range(grid_points):\n",
    "        indexes_best_dih[r] = np.random.choice(starting_pools[r])\n",
    "\n",
    "    temp = sess.run(chi_out, feed_dict = {x_t: traj_ord[indexes_best_dih], keep_prob_drop : 1.})\n",
    "\n",
    "\n",
    "#     temp_samp = sample_from_dist(temp)\n",
    "    predict_cond[:,index_cond] = sess.run(gen_x1, feed_dict = {gen_chi_tau: temp, keep_prob_drop : 1.})[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_gen = md.load_dcd('/group/ag_cmb/scratch/deeptime_data/alanine_data/ala2.dcd', top='/group/ag_cmb/scratch/deeptime_data/alanine_data/ala2.pdb')[:cond_points]\n",
    "idx_heavy = [a for a in range(mol_gen.n_atoms) if mol_gen.topology.atom(a).element.symbol != 'H']\n",
    "mol_gen_small = mol_gen.atom_slice(idx_heavy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dihedral_ind_small = [[1,3,4,6], [3,4,6,8]]\n",
    "dihedrals_cond = np.zeros((cond_points, 6, 2))\n",
    "for i in range(6):\n",
    "    mol_gen_small.xyz = predict_cond[i].reshape(-1, mol_gen_small.n_atoms, 3)\n",
    "    dihedrals_cond[:,i] = md.compute_dihedrals(mol_gen_small, dihedral_ind_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0.5\n",
    "low = -np.pi - offset\n",
    "for r in range(6):\n",
    "    \n",
    "    filt_center =  dihedral[state_centers[r]]\n",
    "    filt_rad = 0.7\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    range_mat = np.array([[-np.pi,np.pi],[-np.pi,np.pi]])\n",
    "\n",
    "    hist_im = ax.hist2d(*dihedrals_cond[:gen_cond_points[r],r].T, bins=100, range = range_mat, norm=matplotlib.colors.LogNorm(), cmap = plt.cm.viridis, normed = True)\n",
    "    ax.scatter(*dihedral[state_centers[r]], s=40, c='C3')\n",
    "    \n",
    "    e = matplotlib.patches.Rectangle(filt_center-filt_rad/2, filt_rad, filt_rad, color='C1')\n",
    "    e.set_clip_box(ax.bbox)\n",
    "    e.set_alpha(0.4)\n",
    "    ax.add_artist(e)\n",
    "    \n",
    "    if r == 5:\n",
    "        new_fc = filt_center-filt_rad/2 + [0, 2*np.pi]\n",
    "        e2 = matplotlib.patches.Rectangle(new_fc, filt_rad, filt_rad, color='C1')\n",
    "        e2.set_clip_box(ax.bbox)\n",
    "        e2.set_alpha(0.4)\n",
    "        ax.add_artist(e2)\n",
    "        \n",
    "    \n",
    "    ax.set_xlim(-np.pi,np.pi)\n",
    "    ax.set_ylim(-np.pi,np.pi)\n",
    "    font = {'fontsize': 14}\n",
    "    ax.set_xticks([-3.1,0,3.1])\n",
    "    ax.set_yticks([-3.1,0,3.1])\n",
    "    ax.set_xticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "    ax.set_yticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "    ax.set_xlabel(r'$\\phi$ [rad]', fontsize = 14)\n",
    "    ax.set_ylabel(r'$\\psi$ [rad]', fontsize = 14)\n",
    "    fig.savefig('cond_dist_gen_state{}.pdf'.format(r), bbox_inches='tight')\n",
    "\n",
    "#     plt.colorbar(hist_im[-1], ax = ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_points = 1\n",
    "cond_points = 250000 -1\n",
    "\n",
    "\n",
    "predict_stat = np.empty((cond_points, 30))\n",
    "predict_stat[0,:] = traj_whole[0]\n",
    "\n",
    "for index_cond in range(1, cond_points):\n",
    "    temp = sess.run(chi_out, feed_dict = {x_t: predict_stat[index_cond-1][None,:], keep_prob_drop : 1.})\n",
    "#     temp_samp = sample_from_dist(temp)\n",
    "    predict_stat[index_cond] = sess.run(gen_x1, feed_dict = {gen_chi_tau: temp, keep_prob_drop : 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_gen = md.load_dcd('/group/ag_cmb/scratch/deeptime_data/alanine_data/ala2.dcd', top='/group/ag_cmb/scratch/deeptime_data/alanine_data/ala2.pdb')[:cond_points]\n",
    "idx_heavy = [a for a in range(mol_gen.n_atoms) if mol_gen.topology.atom(a).element.symbol != 'H']\n",
    "mol_gen_small = mol_gen.atom_slice(idx_heavy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dihedrals_gen = np.zeros((cond_points, 2))\n",
    "\n",
    "mol_gen_small.xyz = predict_stat.reshape(-1, mol_gen_small.n_atoms, 3)\n",
    "dihedrals_gen = md.compute_dihedrals(mol_gen_small, dihedral_ind_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "range_mat = np.array([[-np.pi,np.pi],[-np.pi,np.pi]])\n",
    "\n",
    "hist_im = ax.hist2d(dihedrals_gen[:,0], dihedrals_gen[:,1], bins=300, range = range_mat, norm=matplotlib.colors.LogNorm(), cmap = plt.cm.viridis)\n",
    "ax.scatter(dihedral[state_filt_index,0], dihedral[state_filt_index,1], s=40, c='C3')\n",
    "ax.set_xlim(-np.pi,np.pi)\n",
    "ax.set_ylim(-np.pi,np.pi)\n",
    "if filter_data:\n",
    "    e = matplotlib.patches.Rectangle(*mplt_filt1, color='C1')\n",
    "    e.set_clip_box(ax.bbox)\n",
    "    e.set_alpha(0.2)\n",
    "    ax.add_artist(e)\n",
    "    if mplt_filt2:\n",
    "        e2 = matplotlib.patches.Rectangle(*mplt_filt2, color='C1')\n",
    "        e2.set_clip_box(ax.bbox)\n",
    "        e2.set_alpha(0.2)\n",
    "        ax.add_artist(e2)\n",
    "font = {'fontsize': 14}\n",
    "ax.set_xticks([-3.1,0,3.1])\n",
    "ax.set_yticks([-3.1,0,3.1])\n",
    "ax.set_xticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "ax.set_yticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "ax.set_xlabel(r'$\\phi$ [rad]', fontsize = 14)\n",
    "ax.set_ylabel(r'$\\psi$ [rad]', fontsize = 14)\n",
    "plt.colorbar(hist_im[-1], ax = ax)\n",
    "# fig.savefig('generated_data_withoutKoop_removed_{}state.pdf'.format(state_to_be_filtered), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_points = 1\n",
    "cond_points = length_data\n",
    "\n",
    "\n",
    "predict_stat = np.empty((cond_points, 30))\n",
    "predict_stat[0,:] = traj_whole[0]\n",
    "\n",
    "for index_cond in range(1, cond_points):\n",
    "    temp = sess.run(chi_out, feed_dict = {x_t: predict_stat[index_cond-1][None,:], keep_prob_drop : 1.})\n",
    "    temp_samp = sample_from_dist(temp)\n",
    "    predict_stat[index_cond] = sess.run(gen_x1, feed_dict = {gen_chi_tau: temp_samp, keep_prob_drop : 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_gen = md.load_dcd('/group/ag_cmb/scratch/deeptime_data/ala2.dcd', top='/group/ag_cmb/scratch/deeptime_data/ala2.pdb')[:cond_points]\n",
    "idx_heavy = [a for a in range(mol_gen.n_atoms) if mol_gen.topology.atom(a).element.symbol != 'H']\n",
    "mol_gen_small = mol_gen.atom_slice(idx_heavy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dihedrals_gen = np.zeros((cond_points, 2))\n",
    "\n",
    "mol_gen_small.xyz = predict_stat.reshape(-1, mol_gen_small.n_atoms, 3)\n",
    "dihedrals_gen = md.compute_dihedrals(mol_gen_small, dihedral_ind_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "range_mat = np.array([[-np.pi,np.pi],[-np.pi,np.pi]])\n",
    "\n",
    "hist_im = ax.hist2d(dihedrals_gen[:,0], dihedrals_gen[:,1], bins=300, range = range_mat, norm=matplotlib.colors.LogNorm(), cmap = plt.cm.viridis)\n",
    "ax.scatter(dihedral[state_filt_index,0], dihedral[state_filt_index,1], s=40, c='C3')\n",
    "ax.set_xlim(-np.pi,np.pi)\n",
    "ax.set_ylim(-np.pi,np.pi)\n",
    "e = matplotlib.patches.Rectangle(*mplt_filt1, color='C1')\n",
    "e.set_clip_box(ax.bbox)\n",
    "e.set_alpha(0.2)\n",
    "ax.add_artist(e)\n",
    "if mplt_filt2:\n",
    "    e2 = matplotlib.patches.Rectangle(*mplt_filt2, color='C1')\n",
    "    e2.set_clip_box(ax.bbox)\n",
    "    e2.set_alpha(0.2)\n",
    "    ax.add_artist(e2)\n",
    "font = {'fontsize': 14}\n",
    "ax.set_xticks([-3.1,0,3.1])\n",
    "ax.set_yticks([-3.1,0,3.1])\n",
    "ax.set_xticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "ax.set_yticklabels([r'-$\\pi$','0', r'$\\pi$'], fontdict=font)\n",
    "ax.set_xlabel(r'$\\phi$ [rad]', fontsize = 14)\n",
    "ax.set_ylabel(r'$\\psi$ [rad]', fontsize = 14)\n",
    "plt.colorbar(hist_im[-1], ax = ax)\n",
    "# fig.savefig('generated_data_withoutKoop_removed_{}state.pdf'.format(state_to_be_filtered), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_closest_points = np.argsort(np.linalg.norm(dihedrals_gen[:] - dihedral[state_filt_index], axis = 1))[:100]\n",
    "gen_filter_traj = predict_stat[generated_closest_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_gen_fil = md.load_dcd('/group/ag_cmb/scratch/deeptime_data/ala2.dcd', top='/group/ag_cmb/scratch/deeptime_data/ala2.pdb')[:gen_filter_traj.shape[0]+1]\n",
    "idx_heavy = [a for a in range(mol_gen.n_atoms) if mol_gen.topology.atom(a).element.symbol != 'H']\n",
    "mol_gen_fil_small = mol_gen_fil.atom_slice(idx_heavy)\n",
    "mol_gen_fil_small.xyz = np.concatenate([traj_whole[state_filt_index].reshape(-1,10,3), gen_filter_traj.reshape(-1,10,3)], axis = 0)\n",
    "mol_gen_fil_small.save_pdb('/group/ag_cmb/scratch/deeptime_data/gen_data_withoutKoop_ala_filt_state{}.pdb'.format(state_to_be_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_structures_index = np.argsort(np.linalg.norm(predict_stat - traj_whole[state_filt_index], axis = 1))[:100]\n",
    "closest_structures = predict_stat[closest_structures_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_gen_fil = md.load_dcd('/group/ag_cmb/scratch/deeptime_data/ala2.dcd', top='/group/ag_cmb/scratch/deeptime_data/ala2.pdb')[:closest_structures.shape[0]+1]\n",
    "idx_heavy = [a for a in range(mol_gen.n_atoms) if mol_gen.topology.atom(a).element.symbol != 'H']\n",
    "mol_gen_fil_small = mol_gen_fil.atom_slice(idx_heavy)\n",
    "mol_gen_fil_small.xyz = np.concatenate([traj_whole[state_filt_index].reshape(-1,10,3), closest_structures.reshape(-1,10,3)], axis = 0)\n",
    "mol_gen_fil_small.save_pdb('/group/ag_cmb/scratch/deeptime_data/gen_data_withoutKoop_ala_closest_filt_state{}.pdb'.format(state_to_be_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tf15]",
   "language": "python",
   "name": "conda-env-tf15-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
